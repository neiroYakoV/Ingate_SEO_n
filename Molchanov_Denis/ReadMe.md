27/12/2023:

1. test_17: Gensim LDA (parse "titles") - допилил - в Exp_2 более понятные результаты + визуал (wordcloud);
2. test_19: summarization - LLM on dataset from GensimLDA topics - выбрал подходящую языковую модель, способную понимать и генерировать на русском, нашёл подходящие модули для запуска в гуглколаб, составил "системный промт" для задачи генерации текста из набора слов в датасете, проверил генерацию - работает. Нужно ещё прикрутить настройки "темперутары", top-k, top-p. На GPU T4 работает, потребляет 9ГБ;

26/12/2023:

1. test_17: Gensim LDA (parse "titles") - начал изучение Gensim, тест модели LDA (Латентное распределение Дирихле) - расчёт главной темы (топика) текста на примере собранного набора текстов в test_10 Exp_5 (BeautifulSoup парсер текста по ссылкам) - как итог, топики модель собрирает точно и быстро - далее их можно использовать в генерации осмысленного текста, в качестве итоговой суммаризации объёмного текста (статьи) сайта/сайтов, с помощью генеративной языковой модели (ещё не выбрал модель) - не завершил раздельный расчёт топиков для каждого текста (сайта);
2. test_18: FastText Gensim classification - начал тест встроенной в Gensim модели Fasttext на задаче классификации - интересно сравнить с базовой моделью Fasttext - не завершил;

21/12/2023:
1. test_10 Exp_5: text 'p', 'h4', 'h5', 'h6 parser BeautifulSoup lxml - итоговая таблица BS_Ya_texts_t10_e5;
2. test_16 Exp_1: cointegrated/rut5-base-absum - тест модели суммаризации - модель не подходящая, нужна тяжелее - такие есть - нужно больше времени для экспериментов.

20/12/2023:
1. Начал вести "чендж лог" (историю изменений в colab ноутбуке);
2. test_15: BERT. a small Russian BERT-based encoder "cointegrated/ruBERT-tiny2" - начал изучать возможности классификаций на готовых, дообученных методом "fastBERT" моделей русского языка, провёл 1 эксперимент на простом датасете без дообучения (точность > 90%);
3. test_10 Exp_1: сбор данных в таблицу с BeautifulSoap из Яндекса - добил скрипт до перебора результатов с нескольких страниц поиска и записи всех результатов в датафрейм;
4. test_10 Exp_4: BeautifulSoup lxml button parser - добил скрипт парсера названий всех кнопок из списка сайтов по парсеру из test_10 Exp_1, очистил от мусора, и объединил результаты в один итоговый датафрейм BS_Yandex_results_t10_e1 для дальнейшего анализа и классификации;
5. Визуальное оформление датафрейма с помощью интерактивного data_table google.colab;
6. Удалил некоторые неудачные эксперименты и немного навёл порядок.


14.12.2023:
1. Доработал скрипт "парсинг из Яндекса с помощью BeautifulSoup4" благодаря решению @Uzhegov_Maxim решена проблема частой блокировки обращений в яндекс.
( смотреть test_10 exp_1 в https://colab.research.google.com/drive/1M3jIu4-8Ko0OZV1y-IgGs2A_j-klvQ-S?usp=sharing ) ;
2. Проверил влияние "чистоты" входящей базы данных для обучения нейронной сети на её точность
( смотреть test_14 в https://colab.research.google.com/drive/1M3jIu4-8Ko0OZV1y-IgGs2A_j-klvQ-S?usp=sharing ) ;
3. Убедился в важности разнообразия базы данных обучения модели при проверки на новых данных
( смотреть test_14 в https://colab.research.google.com/drive/1M3jIu4-8Ko0OZV1y-IgGs2A_j-klvQ-S?usp=sharing ) ;

24.11.2023:
Начало сборки кода в гугл-колаб: https://colab.research.google.com/drive/1M3jIu4-8Ko0OZV1y-IgGs2A_j-klvQ-S?usp=sharing

22.11.2023:
Мысли:
1. Кластеризация слов по методу эмбеддинга (Word2Vec ==> Kmeans) - можно попробовать создать эмбеддинг вектор слова/слов: 1)коммерческого плана , и, 2) не коммерческого (информационного) - затем с его помощью классифицировать любой набор слов ;
2. Анонсирована новая модель взаимодействия с поисковой системой - "Google Notes" - "пользователи" сами оставляют заметки о сайте в поисковой выдаче ("поисковая выдача в виде карточек сайта с рейтингом");
3. Из парсеров, скорее всего, интереснее выбрать Scrapy , потому как он "якобы" лучше работает с javascript (современные сайты);
4. "коммерческий/Информационный" запросы - это скорее всего стратегическая опция для клиента: маленький бюджет - смотрим в статистике существующих рекламных позиций, иначе - по просмотрам;
5. Искать среди конкурентов = догонять, плестись в хвосте, гонка бюджетов. Искать новое среди трендов пользователей, создавать релевантные трендам пользователя SEO/ключевики (парсинг хайпов с социалках);

Вопросы:
1. Не привычные коммерческие запросы: "инфоцыгане" ("как скачать - купи урок") - как с этим быть?;
2. Топ поисковой выдачи формируют не только пользователи, но и "Рекламные Кампании", которые не всегда ограничены бюджетом, а потому замусыривают поисковую выдачу - нашу выборку - как с этим быть?;
3. Грамматические ошибки - не все пользователи правильно пишут запросы - как с этим быть?;
#
21.11.2023: мысли по проекту, как вижу порядок реализации:
1. Нужна векторная модель языка по типу Word2vec - кластеризация спарсенного словаря;
#
15.11.2023: мысли по проекту, как вижу порядок реализации:
1. Сбор ключевых-слов из API поисковиков в базу;
2. Создание НС (НС_1) по разделению базы ключевиков на классы
     (Классы: коммерческий, информационный);
4. Анализ сайта клиента на структуру и наполнение методом сравнительного анализа с топ конкурентами через НС (НС_2);
5. Анализ сайта клиента по ключевикам в НС_1;
6. Суммаризация выводов в таблицу (Т_1);
7. НС_3 - языковая модель, с ролью SEO специалиста, анализирует таблицу Т_1 (возможно чат_гпт) и выдаёт понятный ответ с рекомендациями;

#

~~План задач~~ когда-то планировал:
1. ~~Протестировать языковые модели на тему формирования списков по ключевому запросу~~ (безполезное занятие - есть готовые модели слов и семантических векторов);
2. ~~Протестировать openai платформу на тему создания веб приложения~~ (теоретически, возможно, и есть готовые варианты SEO GPT-приложений, но сделать "своё" интереснее);
3. ~~Протестировать nocode платформы по-типу comfyUI на тему SEO приложения~~ (похоже что и гугл-колаб заказчика устроит, а потому нет смысла заморачиваться с интерфейсом, пока ещё);
