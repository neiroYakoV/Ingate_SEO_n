
Для механизма сбора датасета и его автоматизации, учитывая требования определения типа запроса на основе анализа выдачи ТОП-10 конкурентов, я могу предложить следующие варианты:

**Веб-скрейпинг** - это процесс извлечения данных из веб-страниц.
**BeautifulSoup** и **Scrapy** - это две популярные библиотеки для веб-скрейпинга на Python:

1. **BeautifulSoup** - это библиотека Python для парсинга HTML и XML документов. Она часто используется для веб-скрейпинга. BeautifulSoup преобразует сложные HTML-документы в дерево объектов Python, таких как теги, навигабельные строки и так далее.

2. **Scrapy** - это более мощная и гибкая библиотека для веб-скрейпинга, которая также может обрабатывать более сложные задачи, такие как вход в систему, сохранение сессий и следование перекрестным ссылкам.

Парсинг выдачи ТОП-10 страниц: используя библиотеки для парсинга веб-страниц, такие как BeautifulSoup или Scrapy, чтобы получить данные о выдаче ТОП-10 страниц для каждого запроса. И последующее извлечение URL каждой страницы из выдачи.

Анализ URL страниц: Анализ URL каждой страницы из выдачи и проврка наличия ключевых слов или паттернов, указывающих на коммерческую или информационную категорию. 

Определение типа запроса: На основе анализа URL страниц из выдачи, определение типа запроса как коммерческий или информационный. Если большинство страниц содержит коммерческие паттерны в URL, то запрос можно считать коммерческим. Если большинство страниц содержит информационные паттерны в URL, то запрос можно считать информационным.

Автоматизация парсинга и поиска ключевых фраз: Создание скрипта на языке Python, который будет автоматически запускаться и выполнять парсинг выдачи ТОП-10 страниц для каждого запроса. Скрипт должен извлекать URL страниц, анализировать их и определять тип запроса. Затем можно сохранять эти данные в датасет для дальнейшего использования.

Это из возможных вариантов для механизма сбора датасета и его автоматизации, учитывая требования определения типа запроса на основе анализа выдачи ТОП-10 конкурентов. Необходимо конечно ещё настроить и доработать предложенный код. 
